{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load all data\n",
    "#We can use the read_csv() function to load the data and combine the first two columns into a single datetime column to use it as an index.\n",
    "dataset = pd.read_csv('household_power_consumption.txt', sep=';', header=0, low_memory=False, infer_datetime_format=True, parse_dates={'datetime':[0,1]}, index_col=['datetime'])\n",
    "# summarize\n",
    "print(dataset.shape)\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import isnan\n",
    "\n",
    "# mark all missing values\n",
    "dataset.replace('?', np.nan, inplace=True)\n",
    "\n",
    "# make dataset numeric\n",
    "dataset = dataset.astype('float32')\n",
    "\n",
    "# fill missing values with a value at the same time one day ago\n",
    "def fill_missing(values):\n",
    "    one_day = 60 * 24\n",
    "    for row in range(values.shape[0]):\n",
    "        for col in range(values.shape[1]):\n",
    "            if isnan(values[row, col]):\n",
    "                values[row, col] = values[row - one_day, col]\n",
    "                \n",
    "fill_missing(dataset.values)\n",
    "\n",
    "#Alternatively, we can just remove these nan values:\n",
    "#remove columns with nan\n",
    "#dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column for for the remainder of sub metering\n",
    "values = dataset.values\n",
    "dataset['Sub_metering_4'] = (values[:,0] * 1000 / 60) - (values[:,4] + values[:,5] + values[:,6])\n",
    "dataset[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# resample data to daily\n",
    "day_groups = dataset.resample('D')\n",
    "dataset = day_groups.sum()\n",
    "dataset[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "dataset['Global_active_power'].plot(figsize=(13, 5), title = 'Global_active_power', ax = ax)\n",
    "#dataset['Sub_metering_1'].plot(figsize=(,158), title = 'Global_active_power', ax = ax) \n",
    "#dataset['Sub_metering_2'].plot(figsize=(15,8), title = 'Global_active_power', ax = ax) \n",
    "#dataset['Sub_metering_3'].plot(figsize=(15,8), title = 'Global_active_power', ax = ax) \n",
    "#dataset['Sub_metering_4'].plot(figsize=(15,8), title = 'Global_active_power', ax = ax) \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot for each variable\n",
    "plt.figure(figsize=(13,13))\n",
    "for i in range(len(dataset.columns)):\n",
    "    plt.subplot(len(dataset.columns), 1, i+1)\n",
    "    name = dataset.columns[i]\n",
    "    plt.plot(dataset[name])\n",
    "    plt.title(name, y=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "adf_test = adfuller(dataset['Global_active_power'])\n",
    "\n",
    "adf_test\n",
    "\n",
    "# ADF -3.840, p-value = 0.0024\n",
    "#\n",
    "#We can see that the ADF value (the first value in the result) is -3.840) and the p-value (the 2nd value) is 0.0024. \n",
    "#ADF of less than the value of 0.0024 suggests that we can reject the null hypothesis with a significance\n",
    "#level of less than 1% (i.e. a low probability that the result is a statistical fluke). \n",
    "#Rejecting the null hypothesis means that the process has no unit root, and that the time series is \n",
    "#stationary or does not have time-dependent structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.stattools import kpss\n",
    "\n",
    "kpss_test = kpss(dataset['Global_active_power'])\n",
    "\n",
    "kpss_test\n",
    "\n",
    "#Since the p-value is 0.1, the null hypothesis is not rejected at the usual 5% level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import split\n",
    "from numpy import array\n",
    "\n",
    "import sys\n",
    "import numpy\n",
    "numpy.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "#We will use the first three years of data for training predictive models and the final year for evaluating models.\n",
    "#Then, the data is divided into weeks that begin on a Monday and end on a Sunday.\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data):\n",
    "    # split into standard weeks\n",
    "    train, test = data['Global_active_power'][2:-327], data['Global_active_power'][-327:-5] # we take position 2 as this is the first Monday in the dataset\n",
    "    # restructure into windows of weekly data\n",
    "    train = array(split(train, len(train)/7))\n",
    "    test = array(split(test, len(test)/7))\n",
    "    return train, test\n",
    " \n",
    "# splt the dataset\n",
    "train, test = split_dataset(dataset)\n",
    "# check train data\n",
    "print(train.shape)\n",
    "# check test\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# acf and pacf plots of total power\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "from statsmodels.graphics.tsaplots import plot_pacf\n",
    "\n",
    "#We can calculate the correlation for time series observations with the use of obervations of lags. \n",
    "#We can then create a single figure that contains both an ACF and a PACF plot. \n",
    "#The number of lag time steps can be specified, in our case we fix it to 365 days of observations (365 days).\n",
    "#The ACF plot indicates that there is a strong autocorrelation component,\n",
    "#The PACF plot indicates that this component is distinct for the approximatelly 1 lag of observations.\n",
    "\n",
    "# plots\n",
    "pyplot.figure(figsize=(11,7))\n",
    "lags = 365 \n",
    "# acf\n",
    "axis = pyplot.subplot(2, 1, 1)\n",
    "plot_acf(dataset['Global_active_power'], ax=axis, lags=lags)\n",
    "# pacf\n",
    "axis = pyplot.subplot(2, 1, 2)\n",
    "plot_pacf(dataset['Global_active_power'], ax=axis, lags=lags)\n",
    "# show plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "#Since the plots above are quite dense, we can and change the number of lag observations \n",
    "#from 365 to 30 to zoom in the plot.\n",
    "#We can see that a good starting point would be an autoregressive model with 1 lag obervations used as a parameter.\n",
    "\n",
    "lags = 30\n",
    "\n",
    "plot_acf(dataset['Global_active_power'], lags=lags)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cross validation\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits = 3)\n",
    "rmse = []\n",
    "predictions = list()\n",
    "\n",
    "for train_index, test_index in tscv.split(train):\n",
    "    cv_train, cv_test = train[train_index], train[test_index]\n",
    "        \n",
    "    for t in range(len(train)-1):\n",
    "        model = ARIMA(train[t], order=(1,0,0)).fit(disp=False)\n",
    "        yhat = model.predict(len(train[t]), len(train[t])+6)\n",
    "        predictions.append(yhat)\n",
    "        rmse.append(mean_squared_error(train[t+1], yhat))\n",
    "        \n",
    "print(\"rmse score: {}\".format(np.mean(rmse)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMAResults\n",
    "\n",
    "#Running this prints a summary of the fit model. \n",
    "#This summarizes the coefficient values used as BIC and AIC values.\n",
    "\n",
    "print(ARMAResults.summary(model_fit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this cell is not used anymore\n",
    "\n",
    "#from statsmodels.tsa.arima_model import ARIMA\n",
    "#from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#predictions = list()\n",
    "#rmse = list()\n",
    "\n",
    "# arima forecast\n",
    "#for t in range(len(train)-1):\n",
    "    # define the model\n",
    "    #model = ARIMA(train[t], order=(1,0,0))\n",
    "    # fit the model\n",
    "    #model_fit = model.fit(disp=False)\n",
    "    # make forecast\n",
    "    #yhat = model_fit.predict(len(train[t]), len(train[t])+6)\n",
    "    #predictions.append(yhat)\n",
    "    #rmse.append(mean_squared_error(train[t+1], yhat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
